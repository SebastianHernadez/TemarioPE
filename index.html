<!DOCTYPE html>
<head>
    <title >
        TemarioPE
    </title>

    <body>
        <h1>1.- Estadistica descriptiva</h1>
        <h2>1.1 Conceptos básicos de estadística: Definición,
            Teoría de decisión, Población, Muestra aleatoria,
            Parámetros aleatorios.
        </h2>
        <p>
            Definición: Rama de las matemáticas aplicada que estudia la recopilación, organización, análisis e interpretación sistemática de datos.     
        </p>
        <p> 
            Teoría de decisión: Marco teórico que estudia la toma de decisiones bajo condiciones de incertidumbre.
        </p>
        <p>
            Población: Conjunto total de unidades de estudio sobre las cuales se desea inferir alguna característica.
        </p>
        <p>
            Muestra aleatoria: Subconjunto de unidades de la población seleccionado aleatoriamente para su estudio. Representa a la población.
        </p>
        <p>
            Parámetros aleatorios: Variables estadísticas cuya estimación depende de la muestra extraída. Al calcularlos con diferentes muestras, sus valores varían.
        </p>


        <h2> 1.2 Descripción de datos: Datos agrupados y no
            agrupados, Frecuencia de clase, Frecuencia relativa,
            Punto medio, Límites.
        </h2>
        <p> Datos agrupados: Valores agrupados en intervalos de clases definidos.</p>
        <p>Datos no agrupados: Valores individuales no comprendidos en clases.</p>
        <p>Frecuencia de clase: Número de valores contenidos en una clase dada.</p>
        <p>Frecuencia relativa: Frecuencia de clase expresada como porcentaje del total de datos.</p>
        <p>Punto medio: Valor numérico que representa el centro de cada clase en una distribución de frecuencias.</p>
        <p>Límites: Valores que delimitan los extremos inferior y superior de cada una de las clases en una distribución de frecuencias.</p>


        <h2>
            1.3 Medidas de tendencia central: Media aritmética,
            geométrica y ponderada, Mediana, Moda, Medidas de
            dispersión, Varianza, Desviación estándar,
            Desviación media, Desviación mediana, Rango.
        </h2>
        <p>Media aritmética: Promedio algebráico de los valores.</p>
        <p>Media geométrica: Raíz enésima del producto de los valores. Mide centro de datos positivos con distribución logarítmica.</p>
        <p>Media ponderada: Promedio donde cada valor se multiplica por un peso.</p>
        <p>Mediana: Valor central de la distribución cuando los datos están ordenados. Divide la distribución en dos mitades iguales.</p>
        <p>Moda: Valor que más se repite en la distribución.</p>
        <p>Varianza: Medida de dispersión cuadrada promedio respecto a la media.</p>
        <p>Desviación estándar: Raíz cuadrada de la varianza. Expresa dispersión en la misma unidad de medida de los datos.</p>
        <p>Desviación media: Promedio de los valores absolutos de desviación respecto a la media.</p>
        <p>Desviación mediana: Desviación de la mitad de los valores respecto a la mediana.</p>
        <p>Rango: Diferencia entre el máximo y el mínimo valor.</p>


        <h2>
            1.4 Parámetros para datos agrupados.
        </h2>
        <p>
            Los parámetros para datos agrupados son valores estadísticos que se pueden calcular cuando los datos se presentan de forma agrupada en intervalos o clases. Algunos parámetros comúnmente usados con datos agrupados son:

            <p>Frecuencia o tamaño de clase: Número de observaciones en cada intervalo.</p>
            <p>Frecuencia relativa: Porcentaje que representa cada frecuencia con respecto al total.</p>
            <p>Límites de clase: Valores mínimo y máximo que definen cada intervalo.</p>
            <p>Punto medio: Valor que representa el centro de cada intervalo.</p>
            <p>Media: Promedio ponderado de los valores en cada intervalo.</p>
            <p>Moda: Clase o intervalo con mayor frecuencia.</p>
            <p>Mediana: Punto que divide a la muestra en dos partes iguales.</p>
            <p>Varianza y desviación típica: Medidas de dispersión respecto a la media.</p>
            <p>Rango intercuartílico: Amplitud del intervalo que contiene la mediana y la mitad de los datos.</p> 
        </p>


        <h2>
            1.5 Distribución de frecuencias.
        </h2>
        <p>
            La distribución de frecuencias es una tabla o gráfica que presenta de forma ordenada la frecuencia de observación de cada valor de una variable aleatoria. Contiene la siguiente información:

            <p>Clases: Intervalos en que se agrupan los valores de la variable.</p>
            <p>Límites de clases: Valores mínimo y máximo que definen cada intervalo.</p>
            <p>Frecuencias absolutas: Número de observaciones en cada clase.</p>
            <p>Frecuencias relativas: Porcentaje que representa cada frecuencia con respecto al total.</p>
            <p>Puntos medios: Valores representativos de cada intervalo.</p>
        </p>


        <h2>
            1.6 Técnicas de agrupación de datos.
        </h2>
        <p>
            Agrupamiento por igual amplitud: Los intervalos se establecen de igual tamaño. Se usa comúnmente. 
        </p> 
        <p>
            Agrupamiento por números significativos: Los límites de las clases son potencias de 10, 100, etc. Útil para datos altamente diseminados.
        </p>
        <p>
            Raíz cuadrada: El ancho de los intervalos aumenta progresivamente según la raíz cuadrada de su punto medio. Mejor para datos lognormales.
        </p>
        <p>
            Agrupamiento natural: Las clases se definen a partir de puntos significativos en la distribución (edades cada 10 años, temperaturas cada 10°C, etc).
        </p>
        <p>
            Factor de agrupamiento: El ancho de los intervalos aumenta o disminuye según un factor constante predeterminado (2, 5, etc).
        </p>
        <p>
            Agrupamiento por percentiles: Los límites se establecen en los percentiles de la distribución (p25, p50, p75, etc). 
        </p>
        

        <h2>
            1.7 Técnicas de muestreo.
        </h2>
        <p>
            Muestreo aleatorio simple: Cada unidad de la población tiene la misma probabilidad de ser seleccionada, de forma independiente de las demás. 
        </p>
        <p> Muestreo sistemático: Se selecciona la primera unidad al azar y luego se toma cada k-ésima unidad.</p>
        <p>Muestreo estratificado: La población se divide en subsistemas exclusivos y la muestra se selecciona de cada estrato de forma aleatoria.</p>
        <p>Muestreo por conglomerados: La unidad de muestreo es un grupo o conjunto de elementos de la población.</p>
        <p>Muestreo por cuotas: Se determinan cuotas en función de variables clasificatorias en la población.</p>
        <p> Muestreo aleatorio por grupos: Se seleccionan grupos al azar y luego individuos dentro de los grupos.</p>

        <h2>1.8. Histogramas.</h2>
        <p>Un histogramas es una técnica gráfica que permite visualizar la distribución de frecuencias de una variable. Las características principales de un histograma son:

            <p>Eje de las abcisas: Representa los valores o rangos de la variable continua agrupada en intervalos.</p>
            <p>Eje de las ordenadas: Representa las frecuencias absolutas o relativas de cada intervalo.</p>
            <p>Barras: Las barras muestran gráficamente las frecuencias de cada intervalo. Su área es proporcional a la frecuencia.</p>
            <p>Intervalos: División de la escala de valores en rangos contiguos e iguales.</p>
            <p>Forma: La forma del histograma permite ver patrones como simetría, asimetría, moda, etc.</p>
            <p>Área total: Igual a la frecuencia total o al tamaño total de la muestra.</p>
        </p>

        <h1>2.- Fundamentos de la Teoría de Probabilidad.</h1>

        <p>2.1 Técnicas de Conteo</p>
        <ul><li>2.1.1 Principio aditivo.</li> 
            <p>Indica que el total de posibilidades al realizar varios 
                sucesos independientes es el producto de las posibilidades individuales de cada suceso.</p>
            <li>2.1.2 Principio multiplicativo.</li> 
            <p>Indica que si se realizan varias tareas consecutivas de forma independiente, 
                el total de posibilidades es el producto de las posibilidades de cada tarea individual.</p>
            <li>2.1.3 Notación Factorial.</li> 
            <p>Es una forma abreviada para representar el producto de números consecutivos, 
                empezando desde 1 hasta el número dado.</p>
            <li>2.1.4 Permutaciones.</li> 
            <p>Son las diferentes maneras de ordenar los elementos de un grupo de forma que cada elemento salga solo una vez.</p>
            <li>2.1.5 Combinaciones.</li> 
            <p>Son las diferentes maneras de escoger grupos de elementos de un conjunto, sin importar el orden en que están.</p>
            <li>2.1.6 Diagrama de Árbol.</li> 
            <p> Es una forma gráfica de mostrar las diferentes opciones o resultados que se pueden dar al tomar decisiones o enfrentar eventos sucesivos.</p>
            <li>2.1.7 Teorema del Binomio.</li> 
            <p>Da una fórmula para calcular las diferentes agrupaciones posibles al escoger elementos de un grupo de a pocos cada vez, 
                sin repetirlos ni importar el orden.</p>
        </ul>
        <h2>2.2 Teoría elemental de probabilidad.</h2>
        <p>Espacio muestral: Conjunto de todos los resultados posibles de un experimento aleatorio.</p>
        <p>Suceso: Cualquier subconjunto del espacio muestral.</p>
        <p>Probabilidad: Medida numérica del grado de certeza o incerteza de un suceso. Se denota P(A).</p> 
        <p>Sucesos ciertos y imposibles: Un suceso cierto tiene probabilidad 1, uno imposible probabilidad 0.</p> 
        <p>Sucesos mutuamente excluyentes: Dos sucesos que no pueden ocurrir a la vez.</p>  
        <p>Ley de los sucesos complementarios: P(A)+P(A ́)=1, donde A ́ es el suceso complementario de A.</p>  
        <p>Sucesos independientes: La ocurrencia de uno no afecta la probabilidad del otro.</p>    
        <p>Definición clásica de probabilidad: Relación entre el número de casos favorables y el número total de casos posibles.</p>    
        <p>Probabilidad de unión de sucesos: P(A∪B)=P(A)+P(B)-P(A∩B).</p>    
        <p>Probabilidad condicionada: Probabilidad de un suceso dado que otro ya ocurrió. Se denota P(A|B).</p>    
        <p>Teoremas de Bayes: Formula para calcular probabilidades condicionadas.</p>    
        <p>Distribuciones de probabilidad: Modelos matemáticos para calcular probabilidades.</p>   
        
        
        <h2>2.3 Probabilidad de Eventos: Definición de espacio
            muestral, definición de evento, simbología, unión,
            intersección, diagramas de Venn.</h2>
            <p> Espacio muestral: Es el conjunto de todos los resultados posibles de un experimento aleatorio.
                 Se denota generalmente como S.</p>
            <p>Evento: Cualquier subconjunto del espacio muestral. 
                Representa algún resultado o conjunto de resultados del experimento. 
                Se denota con letras mayúsculas como A,B,C.</p>
            <p>Simbología:</p>
            <ul><li>P(A): Probabilidad del evento A.</li> 
                <li> A∪B: Unión de los eventos A y B. Incluye todos los elementos de A o B. </li> 
                <li> A∩B: Intersección de A y B. Incluye solo los elementos comunes a A y B. </li>
            </ul>
            <p>Diagramas de Venn: Representaciones gráficas que muestran las relaciones entre conjuntos mediante círculos.
                 Permiten visualizar uniones e intersecciones.</p>
            <p>Probabilidad de uniones:</p>
                <ul><li> P(A∪B) = P(A) + P(B) - P(A∩B) </li>
                    <li> Se suma la probabilidad de cada evento por separado y se resta la del intersecto para evitar contar dos veces las probabilidades comunes. </li> 
                </ul>
            <p>Propiedades de la probabilidad:</p>
                <ul> <li>P(A) ≥ 0</li> 
                    <li>Si A es cierto, P(A) = 1</li> 
                    <li>P(S) = 1 donde S es el espacio muestral.</li> 
                </ul>
        <h2>2.4 Probabilidad con Técnicas de Conteo: Axiomas,
            Teoremas.</h2>
            <p>Axiomas: Los axiomas fundamentales de la probabilidad son:</p>
            <ul><li>La probabilidad de un suceso está comprendida entre 0 y 1. Es decir, 0 ≤ P(A) ≤ 1</li>
                <li>La probabilidad del espacio muestral completo es 1. Es decir, P(S) = 1</li> 
                <li>La probabilidad de la unión de sucesos disjuntos es la suma de sus probabilidades individuales. Es decir, si A e B son disjuntos, entonces P(A ∪ B) = P(A) + P(B)</li> 
            </ul>

            <p>Teoremas: Algunos teoremas importantes derivados de los axiomas son:</p>
            <ul> <li> Teorema de la probabilidad total: Si A1, A2, ..., An son sucesos mutuamente exclusivos cuya unión constituye el espacio muestral completo, entonces P(S) = P(A1) + P(A2) + ... + P(An) </li>
                 <li>Teorema de Bayes: Relaciona las probabilidades condicionadas de dos sucesos.</li> 
                <li> Teorema de la multiplicación: Si A e B no son sucesos independientes, entonces P(A ∩ B) = P(A) × P(B|A) </li> 
            </ul>

            <p>Técnicas de conteo: Para calcular probabilidades cuando no es posible usar la definición clásica, se usan técnicas como:</p>
            <ul> <li> Regla de la suma: Cuando no se puede contar directamente, se suma el número de casos favorables y posibles. </li> 
                <li>Regla del producto: Para sucesos dependientes se multiplican las probabilidades.</li> 
                <li> Distribuciones de probabilidad discretas: Binomial, de Poisson, hipergeométrica, etc. </li> 
            </ul>
        <h2>2.5 Probabilidad condicional: Dependiente,
            Independiente.</h2>
            <p>Probabilidad condicional:
                La probabilidad de un suceso A dado que ocurre otro suceso B se denota como
                 P(A|B) y se define como P(A|B) = P(A intersect B) / P(B)</p>
                 <p>Dependencia e independencia:</p>
                 <ul><li> Dos sucesos A y B son independientes si la ocurrencia de uno no afecta la probabilidad de ocurrencia del otro, es decir, si P(A|B) = P(A) </li> 
                    <li> De lo contrario, los sucesos son dependientes. </li> 
                </ul>
            <p>Propiedades de la dependencia/independencia:</p>
            <ul><li> Si A e B son independientes, entonces P(A intersect B) = P(A) x P(B) </li>
                <li> Si A e B son dependientes, en general P(A intersect B) ≠ P(A) x P(B) </li>
            </ul>
        <h2>2.6 Ley multiplicativa.</h2>
        <p>La ley multiplicativa es una propiedad importante de la probabilidad condicional que relaciona la probabilidad conjunta de varios eventos con las probabilidades condicionales de cada uno. 
            Establece que:</p>
            <p>P(A y B y C) = P(A) × P(B|A) × P(C|A y B)</p>
            <p>Donde:</p>
            <ul> <li>P(A y B y C) es la probabilidad de que ocurran simultáneamente los eventos A, B y C.</li>
                 <li>P(A) es la probabilidad incondicional del evento A.</li> 
                 <li>P(B|A) es la probabilidad condicional de que ocurra B dado que ya ocurrió A.</li> 
                <li>P(C|A y B) es la probabilidad condicional de que ocurra C dado que ya ocurrieron A y B.</li> 
            </ul>
        <h2>2.7 Eventos independientes: Regla de Bayes.</h2>

        <h1>3.- Variables Aleatorias.</h1>
        <h2>3.1 Variables aleatorias discretas:</h2>
        <p><h3>3.1.1 Distribución de probabilidad en forma general</h3>
            La distribución de probabilidad describe el comportamiento de una variable aleatoria o de un conjunto de variables aleatorias. 
            Proporciona información sobre las posibles valores que puede tomar una variable y las probabilidades asociadas a cada valor.
        </p>
        <h3>3.1.2 Valor esperado</h3>
        <p>El valor esperado, también conocido como esperanza matemática, es un concepto utilizado en teoría de probabilidad y estadística para medir el resultado promedio de un experimento aleatorio. Se representa mediante la letra E y se calcula multiplicando cada posible resultado del experimento por su probabilidad y sumando todos los productos.

            Formalmente, si X es una variable aleatoria discreta con posibles resultados x1, x2, ..., xn y probabilidades correspondientes p1, p2, ..., pn, entonces el valor esperado de X se calcula de la siguiente manera:
            
            <p>E(X) = x1 * p1 + x2 * p2 + ... + xn * pn</p></p>
        <h3>3.1.3 Variancia, desviación estándar.</h3>
        <p>Varianza:
            La varianza se representa comúnmente por el símbolo σ^2 (sigma al cuadrado) y se calcula como la media de los cuadrados de las desviaciones de los valores individuales con respecto a la media.
            La fórmula para calcular la varianza de un conjunto de datos es la siguiente:
            
            <p>σ^2 = [(x1 - μ)^2 + (x2 - μ)^2 + ... + (xn - μ)^2] / n</p>
            
            Donde x1, x2, ..., xn son los valores individuales del conjunto de datos, μ es la media del conjunto de datos y n es el número de elementos en el conjunto de datos.
        </p>
        <p>Desviación estándar:
            La desviación estándar se representa comúnmente por el símbolo σ (sigma) y es simplemente la raíz cuadrada de la varianza. Proporciona una medida de dispersión que está en la misma escala que los datos originales.
            La fórmula para calcular la desviación estándar es la siguiente:
            
            <p>σ = √σ^2</p>
            
            La desviación estándar se utiliza ampliamente debido a que está en la misma escala que los datos originales, lo que facilita la interpretación de su magnitud. Una desviación estándar alta indica una mayor dispersión de los datos,
            mientras que una desviación estándar baja indica una menor dispersión o una mayor concentración de los datos alrededor de la media.
        </p>
        <h3>3.1.4 Función acumulada.</h3>
        <p>La función acumulada, también conocida como función de distribución acumulada (FDA) o función de distribución acumulativa (FDC), es una función que proporciona la probabilidad acumulada de que una variable aleatoria sea menor o igual a un valor dado.
            Formalmente, si X es una variable aleatoria, la función acumulada se define como:
            
            <p>F(x) = P(X ≤ x)</p>
            
            Donde F(x) es la probabilidad acumulada de que X sea menor o igual a x.
            La función acumulada tiene las siguientes propiedades:
            <table> 
                <tr> <td>La función acumulada es no decreciente:</td> 
                    <td>A medida que x aumenta, la probabilidad acumulada también aumenta o se mantiene constante, pero nunca disminuye.</td> 
                </tr> <tr> <td>La función acumulada es acotada:</td> 
                    <td>La probabilidad acumulada está en el rango de 0 a 1, es decir, 0 ≤ F(x) ≤ 1 para cualquier valor de x.</td> </tr>
                     <tr> <td>La función acumulada es continua desde la derecha:</td>
                         <td>La probabilidad acumulada en un punto específico x puede tener discontinuidades, pero la función es continua desde la derecha, lo que significa que el límite de la probabilidad acumulada cuando x se acerca desde la derecha es igual a la probabilidad acumulada en ese punto.</td> </tr> 
            </table>
        </p>
        <h2>3.2 Variables aleatorias Continuas:</h2>
        <h3>3.2.1Distribución de probabilidad en forma general.</h3>
        <p>
            una distribución de probabilidad en forma general se refiere a una clase amplia de distribuciones que pueden describir diferentes tipos de variables aleatorias.

            Una distribución de probabilidad en forma general se caracteriza por su función de densidad de probabilidad (FDP) o su función de masa de probabilidad (FMP),
             dependiendo de si la variable aleatoria es continua o discreta, respectivamente.
            
            Para una variable aleatoria continua, la distribución de probabilidad en forma general se define mediante su función de densidad de probabilidad (FDP). 
            La FDP, generalmente denotada por f(x), describe la probabilidad relativa de que la variable aleatoria tome valores dentro de un intervalo específico. 
            La integral de la FDP sobre un intervalo determinado proporciona la probabilidad de que la variable aleatoria se encuentre dentro de ese intervalo.
        </p>
        <h3>3.2.2 Valor esperado.</h3>
        <p>
            El valor esperado, también conocido como esperanza matemática, es una medida numérica utilizada en teoría de probabilidad y estadística para representar el resultado promedio de una variable aleatoria. Se denota como E(X) o μ.

            El valor esperado de una variable aleatoria discreta se calcula multiplicando cada posible valor de la variable por su probabilidad correspondiente y sumando los productos. Matemáticamente, si X es una variable aleatoria discreta con posibles valores x1, x2, ..., xn y probabilidades correspondientes p1, p2, ..., pn, entonces el valor esperado se calcula de la siguiente manera:

            <p>E(X) = x1 * p1 + x2 * p2 + ... + xn * pn</p>

            En el caso de una variable aleatoria continua, el valor esperado se calcula mediante una integral en lugar de una suma. Si X es una variable aleatoria continua con función de densidad de probabilidad f(x), entonces el valor esperado se calcula de la siguiente manera:

            <p>E(X) = ∫x * f(x) dx</p>

            El valor esperado proporciona una medida de tendencia central de una variable aleatoria. Representa el resultado promedio esperado si se repite el experimento o se observan múltiples muestras de la variable aleatoria. En otras palabras, el valor esperado es el centro de gravedad o punto de equilibrio de la distribución de probabilidad.

            Por ejemplo, si lanzamos un dado justo de seis caras, donde cada cara tiene la misma probabilidad de ocurrencia, el valor esperado del resultado del lanzamiento sería:

            <p>E(X) = (1/6) * 1 + (1/6) * 2 + (1/6) * 3 + (1/6) * 4 + (1/6) * 5 + (1/6) * 6 = 3.5</p>

            Esto significa que, en promedio, se espera obtener un resultado de 3.5 al lanzar el dado repetidamente en un gran número de ocasiones.
        </p>
        <h3>3.2.3 Variancia, desviación estándar.</h3>
        <p>
            Varianza:
                La varianza, denotada comúnmente por σ^2 (sigma al cuadrado), se calcula como la media de los cuadrados de las desviaciones de los valores individuales con respecto al valor esperado.
                La fórmula para calcular la varianza de un conjunto de datos es la siguiente:

                <p>σ^2 = [(x1 - μ)^2 + (x2 - μ)^2 + ... + (xn - μ)^2] / n</p>

                Donde x1, x2, ..., xn son los valores individuales del conjunto de datos, μ es el valor esperado (media) del conjunto de datos y n es el número de elementos en el conjunto de datos.
        </p>
        <p>
            Desviación estándar:
                La desviación estándar, denotada comúnmente por σ (sigma), es simplemente la raíz cuadrada de la varianza. Proporciona una medida de dispersión que está en la misma escala que los datos originales.   
                La fórmula para calcular la desviación estándar es la siguiente:

                <p>σ = √σ^2</p>

                La desviación estándar se utiliza ampliamente debido a que está en la misma escala que los datos originales, lo que facilita la interpretación de su magnitud. 
                Una desviación estándar alta indica una mayor dispersión o variabilidad de los datos, mientras que una desviación estándar baja indica una menor dispersión o una mayor concentración de los datos alrededor del valor esperado.
        </p>
        <h3>3.2.4 Función acumulada.</h3>
        <p>
            La función acumulada, también conocida como función de distribución acumulada (FDA) o cumulative distribution function (CDF) en inglés, es una función utilizada en teoría de probabilidad y estadística para describir la probabilidad acumulada de que una variable aleatoria X sea menor o igual a un valor dado x.

            La función acumulada se denota comúnmente como F(x) o P(X ≤ x), donde X es la variable aleatoria y x es un valor específico.

            Matemáticamente, la función acumulada se define de la siguiente manera:

            <p>F(x) = P(X ≤ x)</p>

            La función acumulada proporciona información sobre cómo se distribuyen los valores de una variable aleatoria en relación con diferentes puntos en la escala de valores. Para cualquier valor dado x, F(x) representa la probabilidad acumulada de que la variable aleatoria X sea menor o igual a x.
        </p>
        <h3>3.2.5 Cálculos de probabilidad.</h3>
        <p>
            El cálculo de probabilidades es una parte fundamental de la teoría de probabilidad y estadística que permite determinar la probabilidad de ocurrencia de eventos o resultados específicos. 
            Aquí hay algunas técnicas y conceptos comunes utilizados en el cálculo de probabilidades:
        </p>
        <table> 
            <tr>
                <td>
                    1.- Regla de la probabilidad:
                    La regla básica de la probabilidad establece que la probabilidad de que ocurra un evento está entre 0 y 1, donde 0 representa la certeza de que el evento no ocurrirá y 1 representa la certeza de que el evento ocurrirá. La probabilidad de un evento se denota como P(A), donde A es el evento en cuestión.
                </td>
                <td>
                    2.- Eventos mutuamente excluyentes:
                    Dos eventos son mutuamente excluyentes si no pueden ocurrir simultáneamente. En este caso, la probabilidad de que ocurra al menos uno de los eventos es la suma de las probabilidades individuales de los eventos. Matemáticamente, para eventos mutuamente excluyentes A y B:
                    <p>P(A o B) = P(A) + P(B)</p>
                </td>
                <td>
                    3.- Eventos independientes:
                    Dos eventos son independientes si la ocurrencia o no ocurrencia de uno de ellos no afecta la probabilidad del otro. En este caso, la probabilidad conjunta de que ambos eventos ocurran es el producto de las probabilidades individuales de los eventos. Matemáticamente, para eventos independientes A y B:
                    <p>P(A y B) = P(A) * P(B)</p>
                </td>
                <td>
                    4.- Regla del complemento:
                    La probabilidad del complemento de un evento A (es decir, la probabilidad de que A no ocurra) se puede calcular restando la probabilidad de A de 1. Matemáticamente:
                    <p>P(A') = 1 - P(A)</p>
                </td>
                <td>
                    5.- Probabilidad condicional:
                    La probabilidad condicional se refiere a la probabilidad de que ocurra un evento dado que otro evento ya ha ocurrido. Se denota como P(A|B), que representa la probabilidad de que ocurra el evento A dado que el evento B ha ocurrido. Matemáticamente:
                    <p>P(A|B) = P(A y B) / P(B)</p>
                </td>
            </tr> 
        </table>

        <h2>Distribuciones de Probabilidad. </h2>
        <h3>4.1 Función de probabilidad.</h3>
        <p>La función de probabilidad es una función que asigna probabilidades a los valores posibles de una variable aleatoria discreta. Indica la probabilidad de que la variable aleatoria tome un valor específico.

            Formalmente, para una variable aleatoria discreta X, la función de probabilidad se define como:
            
            <p>P(X = x) = p(x)</p>
            
            Donde X es la variable aleatoria y x es uno de los posibles valores que puede tomar. 
            La función de probabilidad asigna la probabilidad p(x) al valor x.</p>
        
        
            <h3>4.2 Distribución binomial.</h3>
        <p>La distribución binomial es una distribución de probabilidad discreta que modela el número de éxitos en una serie de ensayos independientes, donde cada ensayo tiene dos posibles resultados: éxito o fracaso. Es una de las distribuciones más utilizadas en estadística y probabilidad debido a su aplicabilidad en una amplia variedad de situaciones.

            Los parámetros clave de la distribución binomial son:
            <ul>
                <il>n: el número total de ensayos o intentos.</il>
                <il>p: la probabilidad de éxito en cada ensayo.</il>
            </ul>
            La función de probabilidad de la distribución binomial se define como:
            
            <p>P(X = k) = C(n, k) * p^k * (1-p)^(n-k)</p>
            
            Donde X es la variable aleatoria que representa el número de éxitos, 
            k es el número de éxitos deseados, 
            C(n, k) es el coeficiente binomial que calcula el número de formas posibles de obtener k éxitos en n ensayos, 
            p es la probabilidad de éxito en cada ensayo y (1-p) es la probabilidad de fracaso en cada ensayo.</p>
            
            
            <h3>4.3 Distribución hipergeométrica.</h3>
            <p>La distribución hipergeométrica es una distribución de probabilidad discreta que modela el número de éxitos en una muestra extraída sin reemplazo de una población finita que contiene un número específico de éxitos y fracasos. A diferencia de la distribución binomial, en la distribución hipergeométrica no hay reposición de los elementos seleccionados, lo que la hace adecuada para situaciones en las que la muestra es una fracción significativa de la población.

                Los parámetros clave de la distribución hipergeométrica son:
                <ul>
                    <li>N: el tamaño total de la población.</li>
                    <li> K: el número total de éxitos en la población.</li>
                    <li>n: el tamaño de la muestra extraída.</li>
                </ul>
                La función de probabilidad de la distribución hipergeométrica se define como:
                
                <p>P(X = k) = (C(K, k) * C(N-K, n-k)) / C(N, n)</p>
                
                Donde X es la variable aleatoria que representa el número de éxitos en la muestra,
                 k es el número de éxitos en la muestra, C(a, b) es el coeficiente binomial que calcula el número de formas posibles de seleccionar b elementos de un conjunto de a elementos y "/" indica la división.</p>

                 <h3>4.4 Distribución de Poisson.</h3>
                 <p>La distribución de Poisson es una distribución de probabilidad discreta que modela la ocurrencia de eventos raros e independientes en un intervalo de tiempo o espacio continuo. Es ampliamente utilizada para describir fenómenos que ocurren de manera aleatoria, como el número de llamadas telefónicas recibidas en un centro de atención, la cantidad de accidentes de tráfico en una intersección o el número de errores en un texto.

                    La distribución de Poisson se caracteriza por un único parámetro, denotado como λ (lambda), que representa la tasa promedio de ocurrencia de los eventos en el intervalo de interés. Los eventos deben ser independientes y la tasa de ocurrencia debe ser constante en todo el intervalo.
                    
                    La función de probabilidad de la distribución de Poisson se define como:
                    
                    <p>P(X = k) = (e^(-λ) * λ^k) / k!</p>
                    
                    Donde X es la variable aleatoria que representa el número de eventos, 
                    k es el número de eventos que se desea calcular la probabilidad, 
                    e es la base del logaritmo natural (aproximadamente 2.71828) y k! es el factorial de k.</p>


                    <h3>4.5 Distribución normal.</h3>
                    <p>La distribución normal, también conocida como distribución de Gauss o distribución gaussiana, es una de las distribuciones de probabilidad más importantes en estadística y probabilidad. Es una distribución continua que se utiliza ampliamente debido a su amplia aplicabilidad y a la presencia frecuente de datos que siguen esta distribución en la vida real.

                        La distribución normal está completamente determinada por dos parámetros: la media (μ) y la desviación estándar (σ). La función de densidad de probabilidad de la distribución normal se define como:
                        
                        <p>f(x) = (1 / (σ * sqrt(2π))) * e^(-(x-μ)^2 / (2σ^2))</p>
                        
                        Donde x es la variable aleatoria, μ es la media, σ es la desviación estándar,
                         π es el número pi (aproximadamente 3.14159) y e es la base del logaritmo natural 
                         (aproximadamente 2.71828).</p>

                    <h3>4.6 Distribución T-student.</h3>
                    <p>La distribución t de Student, también conocida como distribución t de Fisher, es una distribución de probabilidad que se utiliza en inferencia estadística cuando el tamaño de la muestra es pequeño y la desviación estándar de la población es desconocida. Fue desarrollada por el estadístico William Sealy Gosset, quien utilizó el seudónimo "Student" para publicar sus investigaciones debido a las restricciones de confidencialidad de su empleador en la cervecería Guinness.

                        La distribución t de Student se caracteriza por un parámetro llamado grados de libertad (df), que está relacionado con el tamaño de la muestra. Cuanto menor sea el tamaño de la muestra, mayor será la variabilidad de la distribución t. A medida que el tamaño de la muestra aumenta, la distribución t se aproxima a la distribución normal estándar.
                        
                        La función de densidad de probabilidad de la distribución t de Student se define como:
                        
                        <p>f(x) = (1 / (sqrt(df) * B(1/2, df/2))) * (1 + (x^2 / df))^(-(df+1) / 2)</p>
                        
                        Donde x es la variable aleatoria, df son los grados de libertad y B es la función beta.</p>
                    
                    <h3>4.7 Distribución Chi cuadrada.</h3>
                    <p>La distribución chi cuadrada, también conocida como distribución chi-cuadrado o χ² (chi-cuadrado), es una distribución de probabilidad continua que se utiliza en estadística para analizar la varianza y realizar pruebas de bondad de ajuste.

                        La distribución chi cuadrada se obtiene al sumar el cuadrado de variables aleatorias independientes estándar normales (con media cero y varianza uno). El parámetro clave de esta distribución es el número de grados de libertad (df), que determina su forma y características.
                        
                        La función de densidad de probabilidad de la distribución chi cuadrada se define como:
                        
                        <p>f(x) = (1 / (2^(df/2) * Γ(df/2))) * (x^(df/2 - 1)) * e^(-x/2)</p>
                        
                        Donde x es la variable aleatoria, df son los grados de libertad, 
                        Γ es la función gamma y e es la base del logaritmo natural (aproximadamente 2.71828).</p>
                    
                    <h3>4.8 Distribución F.</h3>
                    <p>La distribución F, también conocida como distribución F de Fisher-Snedecor, es una distribución de probabilidad continua que se utiliza en estadística para comparar la varianza de dos muestras y realizar pruebas de hipótesis en análisis de varianza.

                        La distribución F se basa en dos parámetros llamados grados de libertad: df1 y df2. Los grados de libertad df1 se asocian con la varianza de la primera muestra, mientras que los grados de libertad df2 se asocian con la varianza de la segunda muestra. Estos parámetros determinan la forma y las características de la distribución F.
                        
                        La función de densidad de probabilidad de la distribución F se define como:
                        
                        <p>f(x) = (1 / (B(df1/2, df2/2) * (df1/df2)^(df1/2) * x^((df1/2)-1) * (1 + ((df1/df2) * x))^(-(df1+df2)/2)</p>
                        
                        Donde x es la variable aleatoria, df1 y df2 son los grados de libertad y B es la función beta.</p>
    </body>
</head>
</html>